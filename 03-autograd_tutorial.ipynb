{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
    "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an operation on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Deduct 2 from all elements\n",
    "y = x - 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x7f6533465430>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# What's happening here?\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubBackward0 at 0x7f6533465640>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's dig further...\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metadata', 'name', 'next_functions', 'register_hook', 'requires_grad']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in dir(y.grad_fn) if not s.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, <function SubBackward0.name>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.metadata, y.grad_fn.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function SubBackward0.register_hook>, True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.register_hook, y.grad_fn.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7f65334ccfa0>, 0), (None, 0))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x7f65334ccfa0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metadata',\n",
       " 'name',\n",
       " 'next_functions',\n",
       " 'register_hook',\n",
       " 'requires_grad',\n",
       " 'variable']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ s for s in dir(y.grad_fn.next_functions[0][0]) if not s.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].variable is x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It seems that `torch` uses this `next_functions` to keep going deeper until there is no more `next_functions` like the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  0.],\n",
      "        [ 3., 12.]], grad_fn=<MulBackward0>)\n",
      "tensor(4.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "a = z.mean()  # average\n",
    "\n",
    "print(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7f6533465be0>, 0), (None, 0))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<SubBackward0 at 0x7f6533465640>, 0), (<SubBackward0 at 0x7f6533465640>, 0))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions[0][0].next_functions[0][0] is \\\n",
    "z.grad_fn.next_functions[0][0].next_functions[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions[0][0].next_functions[0][0] is y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7f6532c2e1f0>, 0),)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad_fn.next_functions[0][0] is z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise the computational graph! (thks @szagoruyko)\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.47.3 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"106pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 106.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-267 102,-267 102,4 -4,4\"/>\n",
       "<!-- 140072620067280 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140072620067280</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"98,-21 0,-21 0,0 98,0 98,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140072620057072 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140072620057072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94.5,-78 3.5,-78 3.5,-57 94.5,-57 94.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140072620057072&#45;&gt;140072620067280 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140072620057072&#45;&gt;140072620067280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49,-56.92C49,-49.91 49,-40.14 49,-31.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.5,-31.34 49,-21.34 45.5,-31.34 52.5,-31.34\"/>\n",
       "</g>\n",
       "<!-- 140072628673504 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140072628673504</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94.5,-135 3.5,-135 3.5,-114 94.5,-114 94.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140072628673504&#45;&gt;140072620057072 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140072628673504&#45;&gt;140072620057072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49,-113.92C49,-106.91 49,-97.14 49,-88.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.5,-88.34 49,-78.34 45.5,-88.34 52.5,-88.34\"/>\n",
       "</g>\n",
       "<!-- 140072628672064 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140072628672064</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-192 4,-192 4,-171 94,-171 94,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140072628672064&#45;&gt;140072628673504 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140072628672064&#45;&gt;140072628673504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.73,-170.92C42.35,-163.91 41.94,-154.14 42.48,-145.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"45.98,-145.68 43.68,-135.34 39.03,-144.86 45.98,-145.68\"/>\n",
       "</g>\n",
       "<!-- 140072628672064&#45;&gt;140072628673504 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140072628672064&#45;&gt;140072628673504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.27,-170.92C55.65,-163.91 56.06,-154.14 55.52,-145.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"58.97,-144.86 54.32,-135.34 52.02,-145.68 58.97,-144.86\"/>\n",
       "</g>\n",
       "<!-- 140072629096352 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140072629096352</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"76,-263 22,-263 22,-228 76,-228 76,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 140072629096352&#45;&gt;140072628672064 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140072629096352&#45;&gt;140072628672064</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49,-227.89C49,-219.99 49,-210.5 49,-202.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.5,-202.02 49,-192.02 45.5,-202.02 52.5,-202.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f6532c30a30>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.47.3 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"99pt\" height=\"100pt\"\n",
       " viewBox=\"0.00 0.00 99.00 100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 96)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-96 95,-96 95,4 -4,4\"/>\n",
       "<!-- 140072629073232 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140072629073232</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"91,-21 0,-21 0,0 91,0 91,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140072629096352 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140072629096352</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"72.5,-92 18.5,-92 18.5,-57 72.5,-57 72.5,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 140072629096352&#45;&gt;140072629073232 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140072629096352&#45;&gt;140072629073232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M39.44,-56.89C38.62,-48.9 38.53,-39.29 39.18,-30.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"42.66,-31.38 40.47,-21.02 35.72,-30.49 42.66,-31.38\"/>\n",
       "</g>\n",
       "<!-- 140072629096352&#45;&gt;140072629073232 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140072629096352&#45;&gt;140072629073232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.56,-56.89C52.38,-48.9 52.47,-39.29 51.82,-30.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.28,-30.49 50.53,-21.02 48.34,-31.38 55.28,-30.49\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f65334c7790>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.47.3 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"99pt\" height=\"100pt\"\n",
       " viewBox=\"0.00 0.00 99.00 100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 96)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-96 95,-96 95,4 -4,4\"/>\n",
       "<!-- 140072629063152 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140072629063152</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"91,-21 0,-21 0,0 91,0 91,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140072629096352 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140072629096352</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"72.5,-92 18.5,-92 18.5,-57 72.5,-57 72.5,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 140072629096352&#45;&gt;140072629063152 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140072629096352&#45;&gt;140072629063152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.5,-56.89C45.5,-48.99 45.5,-39.5 45.5,-31.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"49,-31.02 45.5,-21.02 42,-31.02 49,-31.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f65334c46d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(1.5*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop\n",
    "a.backward()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/home/phunc20/.config/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
    "  Variable._execution_engine.run_backward("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?1)**\n",
    "Please solve this prolem of `Found no NVIDIA driver`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cpuonly` package is needed. cf. the edited `environment.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5000,  0.0000],\n",
      "        [ 1.5000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Compute it by hand BEFORE executing this\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?2)** Deduce the same numbers/result by hand calculating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(R2)**\n",
    "![](figs/note-autodiff.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5000,  0.0000],\n",
       "        [ 1.5000,  3.0000]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-1ee473f924b8>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  z.grad\n"
     ]
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-10b3a7061f6d>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?)** How do we do in order to see the partial derivatives of `a` w.r.t. `z` and `y`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do many crazy things with autograd!\n",
    "> With Great *Flexibility* Comes Great Responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -961.6556, -1200.7402,   943.9127], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic graphs!\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    # frobenius norm, i.e. matrix viewed as vector\n",
    "    # enfin, je me corrige, ici il s'agit du 2-norme du vecteur y\n",
    "    y = y * 2\n",
    "    # at each iteration, we double the length of the vector y\n",
    "    i += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# If we don't run backward on a scalar we need to specify the grad_output\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?3)** What does it mean to have `y.backward(gradients)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(R3)**\n",
    "A function\n",
    "$f: \\quad\n",
    "\\begin{align}\n",
    "  \\mathbb{R}^{n} &\\to \\mathbb{R}^{m} \\\\\n",
    "  (x_1,x_2,\\ldots,x_n) &\\mapsto (y_1,y_2,\\ldots,y_m) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "admits jacobian\n",
    "\n",
    "$$\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}.\n",
    "$$\n",
    "\n",
    "Loosely speaking, when we have a vector $\\mathbf{v} = \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$, **gradient of some scalar function** $l: \\mathbb{R}^{m} \\to \\mathbb{R}$, the gradient of the composite function $l \\circ f$ equals\n",
    "$$\n",
    "J^{T} \\mathbf{v} =\n",
    "\\begin{align}\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\\begin{pmatrix}\n",
    "  \\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    "  %\\frac{\\partial l}{\\partial y_{2}} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial l}{\\partial y_{m}} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "To answer the question, `y.backward(gradients)` returns $J^{T} \\mathbf{v}$, where $J$ is the jacobian (of `y` as a function of `x`) and $\\mathbf{v} =$ `gradients`.\n",
    "\n",
    "#### Let's check this with `x.grad`\n",
    "The jacobian in this case is $\\left(y_{i} = 2^{k} x_{i}\\quad\\forall\\;\\; i=1,2,3\\right)$\n",
    "\n",
    "$$\n",
    "J = \\begin{pmatrix}\n",
    "2^{k} & 0     & 0 \\\\\n",
    "0     & 2^{k} & 0 \\\\\n",
    "0     & 0     & 2^{k} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "With $v = \\begin{pmatrix} 0.1 \\\\ 1.0 \\\\ 0.0001 \\end{pmatrix}$, we have $J^{T} v = \\begin{pmatrix} 0.1\\times 2^{k} \\\\ 1.0 \\times 2^{k} \\\\ 0.0001 \\times 2^{k} \\end{pmatrix}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE executing this, can you tell what would you expect it to print?\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients * (2**(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(gradients * (2**(i+1)), x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1140, -1.6601, -0.4037], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m See :func:`torch.norm`\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.config/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/tensor.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.data.norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable decides the tensor's range below\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Both x and w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1, requires_grad=True)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x  # inner product of two vectors w and x\n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1., 2., 3.], requires_grad=True)\n",
      "w = tensor([1., 1., 1.], requires_grad=True)\n",
      "z = 6.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"z = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Only w that allows gradient accumulation:\n",
    "# By default, requires_grad=False.\n",
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x\n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError!!! >:[\n",
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "\n",
    "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
    "with torch.no_grad():\n",
    "    z = w @ x\n",
    "\n",
    "try:\n",
    "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
    "except RuntimeError as e:\n",
    "    print('RuntimeError!!! >:[')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-1927a4498105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.config/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.config/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.config/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "w = torch.ones(n, requires_grad=True)\n",
    "w.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More stuff\n",
    "\n",
    "Documentation of the automatic differentiation package is at\n",
    "http://pytorch.org/docs/autograd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
